{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05_Webcam_Demo.ipynb\n",
    "### Real-time Facial Emotion Recognition Demo (Webcam)\n",
    "This notebook demonstrates running the webcam demo locally. It uses MTCNN for face detection and a trained Keras model for emotion classification. For best results run this notebook in a local environment (not cloud notebook) where a webcam is available.\n",
    "If running inside Jupyter, the video window will open via OpenCV (`cv2.imshow`). Press **q** in the video window to quit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once in the environment)\n",
    "!pip install mtcnn opencv-python tensorflow pretty_midi numpy --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV version: 4.12.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from datetime import datetime\n",
    "import warnings, absl.logging\n",
    "\n",
    "print('OpenCV version:', cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters - edit as needed\n",
    "MODEL_PATH = '../models/mobilenet_emotion.h5'   # path to your trained model\n",
    "DATA_DIR = '../data/cropped_faces'              # used to infer labels (train subfolder names)\n",
    "INPUT_SIZE = 224                             # model input size\n",
    "CAMERA_INDEX = 0                             # change if you have multiple cameras\n",
    "GENERATE_MIDI = False                         # set True to save MIDI when emotion changes\n",
    "MIDI_OUT_DIR = 'outputs/generated_music'\n",
    "os.makedirs(MIDI_OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['0', '1', '2', '3', '4', '5', '6']\n"
     ]
    }
   ],
   "source": [
    "def get_labels_from_train_dir(train_dir):\n",
    "    if not os.path.isdir(train_dir):\n",
    "        raise ValueError(f\"Train dir not found: {train_dir}\")\n",
    "    labels = sorted([d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))])\n",
    "    return labels\n",
    "\n",
    "labels = get_labels_from_train_dir(os.path.join(DATA_DIR, 'train'))\n",
    "print('Labels:', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "print('Loading model...')\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting webcam. Press q in the video window to quit.\n",
      "Webcam demo ended.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "\n",
    "try:\n",
    "    tf.keras.utils.disable_interactive_logging()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "tqdm.tqdm = lambda *a, **k: a[0] if a else None\n",
    "detector = MTCNN()\n",
    "cap = cv2.VideoCapture(CAMERA_INDEX)\n",
    "prev_label = None\n",
    "\n",
    "print('Starting webcam. Press q in the video window to quit.')\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print('Failed to read from camera. Exiting.')\n",
    "            break\n",
    "\n",
    "        # Face detection\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        detections = detector.detect_faces(rgb)\n",
    "        for det in detections:\n",
    "            x, y, w, h = det['box']\n",
    "            x, y = max(0, x), max(0, y)\n",
    "            face = frame[y:y+h, x:x+w]\n",
    "\n",
    "            try:\n",
    "                face_resized = cv2.resize(face, (INPUT_SIZE, INPUT_SIZE))\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            # Normalize and predict emotion\n",
    "            face_arr = face_resized.astype('float32') / 255.0\n",
    "            face_arr = np.expand_dims(face_arr, axis=0)\n",
    "            preds = model.predict(face_arr, verbose=0)\n",
    "            idx = int(np.argmax(preds))\n",
    "            prob = float(np.max(preds))\n",
    "            label = labels[idx] if idx < len(labels) else str(idx)\n",
    "\n",
    "            # Draw label and bounding box\n",
    "            color = (0, 255, 0) if prob > 0.6 else (0, 200, 200)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, f\"{label} {prob:.2f}\", (x, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2)\n",
    "\n",
    "            # Optional: generate MIDI on emotion change\n",
    "            if GENERATE_MIDI and label != prev_label and prob > 0.6:\n",
    "                try:\n",
    "                    from scripts.emotion_to_midi import generate_melody\n",
    "                    midi_path = os.path.join(MIDI_OUT_DIR,\n",
    "                        f\"{label}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mid\")\n",
    "                    generate_melody(label, length=16, out_path=midi_path)\n",
    "                    print('Saved MIDI:', midi_path)\n",
    "                except Exception as e:\n",
    "                    print('MIDI generation failed:', e)\n",
    "                prev_label = label\n",
    "\n",
    "        # Show live feed\n",
    "        cv2.imshow('FER Webcam Demo', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print('Webcam demo ended.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "- If the webcam window doesn't open in your environment, run `python scripts/webcam_demo.py` from a terminal instead of the notebook.\n",
    "- If MTCNN is slow, consider using OpenCV Haar cascade for face detection as a faster (but less accurate) alternative.\n",
    "- If model loading fails, ensure `MODEL_PATH` points to a valid Keras `.h5` or SavedModel.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.19",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
